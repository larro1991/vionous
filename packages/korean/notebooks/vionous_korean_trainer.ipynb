{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vionous Korean Language LoRA Trainer\n",
        "\n",
        "Train a LoRA adapter to teach Qwen2.5-7B about korean language using the Vionous Korean Language knowledge package.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with T4 GPU (free tier works)\n",
        "- ~1-2 hours runtime\n",
        "\n",
        "**Output:**\n",
        "- LoRA adapter files (adapter_model.safetensors, adapter_config.json)\n",
        "- Downloadable as zip\n",
        "\n",
        "---\n",
        "**Before running:** Go to Runtime → Change runtime type → Select T4 GPU"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q transformers>=4.36.0\n",
        "!pip install -q peft>=0.7.0\n",
        "!pip install -q datasets>=2.14.0\n",
        "!pip install -q bitsandbytes>=0.41.0\n",
        "!pip install -q trl>=0.7.0\n",
        "!pip install -q accelerate>=0.25.0\n",
        "!pip install -q scipy\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Clone vionous repo and load training data\n",
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Clone the repository\n",
        "if not os.path.exists('vionous'):\n",
        "    !git clone https://github.com/larro1991/vionous.git\n",
        "    print(\"Repository cloned!\")\n",
        "else:\n",
        "    print(\"Repository already exists\")\n",
        "\n",
        "# Load training data\n",
        "DATA_PATH = \"vionous/packages/korean/training-data\"\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_data = load_jsonl(f\"{DATA_PATH}/train.jsonl\")\n",
        "val_data = load_jsonl(f\"{DATA_PATH}/val.jsonl\")\n",
        "\n",
        "print(f\"\\nLoaded {len(train_data):,} training examples\")\n",
        "print(f\"Loaded {len(val_data):,} validation examples\")\n",
        "\n",
        "# Convert to chat format for Qwen\n",
        "def format_for_training(example):\n",
        "    return {\n",
        "        \"text\": f\"<|im_start|>user\\n{example['question']}<|im_end|>\\n<|im_start|>assistant\\n{example['answer']}<|im_end|>\"\n",
        "    }\n",
        "\n",
        "train_formatted = [format_for_training(ex) for ex in train_data]\n",
        "val_formatted = [format_for_training(ex) for ex in val_data]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_list(train_formatted)\n",
        "val_dataset = Dataset.from_list(val_formatted)\n",
        "\n",
        "print(f\"\\nDatasets created!\")\n",
        "print(f\"Sample:\\n{train_formatted[0]['text'][:300]}...\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load base model with 4-bit quantization\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading {MODEL_ID} with 4-bit quantization...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "print(f\"Device: {model.device}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Configure LoRA\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / all_param:.2f}%)\")\n",
        "    print(f\"All params: {all_param:,}\")\n",
        "\n",
        "print(\"\\nLoRA Configuration:\")\n",
        "print(f\"  r: {lora_config.r}\")\n",
        "print(f\"  alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  target_modules: {lora_config.target_modules}\")\n",
        "print()\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "configure_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Set up trainer\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "OUTPUT_DIR = \"./vionous-korean-lora\"\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    max_grad_norm=0.3,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured!\")\n",
        "print(f\"\\nTraining settings:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {
        "id": "setup_trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Train!\n",
        "print(\"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nThis will take approximately 1-2 hours on a T4 GPU.\")\n",
        "print(\"You can monitor progress below.\\n\")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Save adapter\n",
        "ADAPTER_DIR = \"./vionous-korean-adapter\"\n",
        "\n",
        "# Save the LoRA adapter\n",
        "model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "\n",
        "print(f\"Adapter saved to: {ADAPTER_DIR}\")\n",
        "print(\"\\nFiles created:\")\n",
        "!ls -la {ADAPTER_DIR}"
      ],
      "metadata": {
        "id": "save_adapter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Test with korean language questions\n",
        "print(\"=\"*50)\n",
        "print(\"TESTING THE TRAINED MODEL\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "test_questions = [\n",
        "    \"How do I learn Hangul?\",\n",
        "    \"What are honorifics?\",\n",
        "    \"How do I use particles?\",\n",
        "    \"What's the difference between 은/는 and 이/가?\",\n",
        "    \"How do I conjugate verbs?\",\n",
        "]\n",
        "\n",
        "def generate_response(question):\n",
        "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1]\n",
        "        response = response.replace(\"<|im_end|>\", \"\").strip()\n",
        "    return response\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    answer = generate_response(q)\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Download adapter as zip\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "ZIP_NAME = \"vionous-korean-lora-adapter\"\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive(ZIP_NAME, 'zip', ADAPTER_DIR)\n",
        "\n",
        "print(f\"Created: {ZIP_NAME}.zip\")\n",
        "print(\"\\nDownloading...\")\n",
        "\n",
        "# Download\n",
        "files.download(f\"{ZIP_NAME}.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ALL DONE!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nYour Korean Language LoRA adapter has been downloaded.\")"
      ],
      "metadata": {
        "id": "download_adapter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Vionous Knowledge Package** | Korean Language | CC-BY-SA 4.0 | Source: Stack Exchange"
      ],
      "metadata": {
        "id": "footer"
      }
    }
  ]
}